function [Theta1, Theta2] = getTheta(X, y, lambda)

options = optimset('MaxIter', 50);

%shorthand for neuralcostfunc, that only takes in the nn param as input
costFunction = @(p) neuralCostFunc(p, X, y, lambda);

%random initial thetas to to avoid symmetry issue
init_Theta1 = initialWeights(input_layer_size, hidden_layer_size);
init_Theta2 = initialWeights(hidden_layer_size, num_labels);

init_Theta1 = rand(2, 3) * 2 * 0.12 - epsil;


init_nn_params = [init_Theta1(:) ; init_Theta2(:)];

%find theta by minimizing cost function
[theta, ~] = fmincg(costFunction, init_nn_params, options);

Theta1 = reshape(theta(1:hidden_layer_size * (input_layer_size + 1)), hidden_layer_size, (input_layer_size + 1));
Theta2 = reshape(theta((1 + (hidden_layer_size * (input_layer_size + 1))):end), num_labels, (hidden_layer_size + 1));


end
